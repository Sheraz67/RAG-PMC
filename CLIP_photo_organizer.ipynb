{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sheraz67/RAG-PMC/blob/main/CLIP_photo_organizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gqPpSIm6k8B",
        "outputId": "25b2dbc7-058d-491a-8920-7844af45dc99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Code successfully saved to app.py\n"
          ]
        }
      ],
      "source": [
        "code = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import pickle\n",
        "from pillow_heif import register_heif_opener\n",
        "\n",
        "# Register HEIF opener\n",
        "register_heif_opener()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "st.write(f\"Using device: {device}\")\n",
        "\n",
        "# Load CLIP model and processor\n",
        "@st.cache_resource\n",
        "def load_clip_model():\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").to(device)\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
        "    return model, processor\n",
        "\n",
        "clip_model, clip_processor = load_clip_model()\n",
        "\n",
        "# Cache file for embeddings\n",
        "CACHE_FILE = \"embeddings.pkl\"\n",
        "\n",
        "# Encode images\n",
        "def encode_images(uploaded_files):\n",
        "    paths, embeddings = [], []\n",
        "    for file in uploaded_files:\n",
        "        try:\n",
        "            image = Image.open(file)\n",
        "            image = image.convert(\"RGB\")\n",
        "            inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.get_image_features(**inputs)\n",
        "            paths.append(file.name)\n",
        "            embeddings.append(embedding[0].cpu().numpy())\n",
        "            st.write(f\"Encoded: {file.name}\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error processing {file.name}: {e}\")\n",
        "\n",
        "    if paths:\n",
        "        with open(CACHE_FILE, \"wb\") as f:\n",
        "            pickle.dump((paths, embeddings), f)\n",
        "        st.success(\"Images encoded and cached!\")\n",
        "    return paths, embeddings\n",
        "\n",
        "# Process text query\n",
        "def process_query(query):\n",
        "    refined_query = f\"a photo of {query}\"\n",
        "    inputs = clip_processor(text=refined_query, return_tensors=\"pt\", padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        embedding = clip_model.get_text_features(**inputs)\n",
        "    return embedding[0].cpu().numpy()\n",
        "\n",
        "# Find matches\n",
        "def find_matches(query_embedding, image_embeddings, image_paths, top_k=3):\n",
        "    similarities = [\n",
        "        np.dot(query_embedding, img_emb) /\n",
        "        (np.linalg.norm(query_embedding) * np.linalg.norm(img_emb))\n",
        "        for img_emb in image_embeddings\n",
        "    ]\n",
        "    sorted_pairs = sorted(zip(image_paths, similarities), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_pairs[:top_k]\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Text-Driven Photo Organizer\")\n",
        "st.markdown(\"Upload your photos and search them with natural language!\")\n",
        "\n",
        "# File uploader\n",
        "uploaded_files = st.file_uploader(\"Upload Photos\", accept_multiple_files=True, type=[\"jpg\", \"jpeg\", \"png\", \"heic\", \"HEIC\"])\n",
        "\n",
        "# Process uploaded files\n",
        "if uploaded_files:\n",
        "    if os.path.exists(CACHE_FILE):\n",
        "        cached_files = pickle.load(open(CACHE_FILE, \"rb\"))[0]\n",
        "        if sorted([f.name for f in uploaded_files]) != sorted(cached_files):\n",
        "            os.remove(CACHE_FILE)\n",
        "            st.write(\"New files detected, re-encoding...\")\n",
        "\n",
        "    if not os.path.exists(CACHE_FILE):\n",
        "        paths, embeddings = encode_images(uploaded_files)\n",
        "    else:\n",
        "        with open(CACHE_FILE, \"rb\") as f:\n",
        "            paths, embeddings = pickle.load(f)\n",
        "        st.write(\"Loaded cached embeddings.\")\n",
        "\n",
        "    query = st.text_input(\"Search your photos (e.g., 'sunset')\", \"\")\n",
        "    if query:\n",
        "        query_embedding = process_query(query)\n",
        "        matches = find_matches(query_embedding, embeddings, paths)\n",
        "\n",
        "        st.subheader(f\"Top {len(matches)} Matches for '{query}'\")\n",
        "        cols = st.columns(min(3, len(matches)))\n",
        "\n",
        "        for i, (path, score) in enumerate(matches):\n",
        "            col = cols[i % len(cols)]\n",
        "            file_obj = next((f for f in uploaded_files if f.name == path), None)\n",
        "\n",
        "            if file_obj:\n",
        "                caption_text = path + \" | Score: \" + \"{:.4f}\".format(score)\n",
        "                col.image(file_obj, caption=caption_text, width=200)\n",
        "            else:\n",
        "                st.error(\"File not found: \" + path)\n",
        "\n",
        "# Reset button\n",
        "if st.button(\"Reset Cache\"):\n",
        "    if os.path.exists(CACHE_FILE):\n",
        "        os.remove(CACHE_FILE)\n",
        "        st.success(\"Cache cleared! Upload new photos to start fresh.\")\n",
        "'''\n",
        "\n",
        "# Save the code to app.py\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"✅ Code successfully saved to app.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X8d1zZ87i15"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501 --allow-invalid-certy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECJNRLq_HUzB",
        "outputId": "ded5583a-7d05-4856-8b97-abbd42e783f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.75.239.177"
          ]
        }
      ],
      "source": [
        "!curl https://loca.lt/mytunnelpassword\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOKzgDf9Y2luZ1aFRrwsPZ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}